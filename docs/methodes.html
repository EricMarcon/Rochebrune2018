<!DOCTYPE html>
<html  lang="french">

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Mesure de la biodiversité et de la structuration spatiale de l’activité économique par l’entropie</title>
  <meta name="description" content="Mesure de la biodiversité et de la structuration spatiale de l’activité économique par l’entropie">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Mesure de la biodiversité et de la structuration spatiale de l’activité économique par l’entropie" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Mesure de la biodiversité et de la structuration spatiale de l’activité économique par l’entropie" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="concentration-spatiale-et-specialisation.html">
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="methodes.html"><a href="methodes.html"><i class="fa fa-check"></i><b>2</b> Méthodes</a><ul>
<li class="chapter" data-level="2.1" data-path="methodes.html"><a href="methodes.html#questions-similaires-et-notions-opposees"><i class="fa fa-check"></i><b>2.1</b> Questions similaires et notions opposées</a></li>
<li class="chapter" data-level="2.2" data-path="methodes.html"><a href="methodes.html#donnees-et-notations"><i class="fa fa-check"></i><b>2.2</b> Données et notations</a></li>
<li class="chapter" data-level="2.3" data-path="methodes.html"><a href="methodes.html#lentropie-comme-mesure-dincertitude"><i class="fa fa-check"></i><b>2.3</b> L’entropie comme mesure d’incertitude</a><ul>
<li class="chapter" data-level="2.3.1" data-path="methodes.html"><a href="methodes.html#entropie-de-shannon"><i class="fa fa-check"></i><b>2.3.1</b> Entropie de Shannon</a></li>
<li class="chapter" data-level="2.3.2" data-path="methodes.html"><a href="methodes.html#entropie-generalisee"><i class="fa fa-check"></i><b>2.3.2</b> Entropie généralisée</a></li>
<li class="chapter" data-level="2.3.3" data-path="methodes.html"><a href="methodes.html#de-lentropie-a-la-diversite"><i class="fa fa-check"></i><b>2.3.3</b> De l’entropie à la diversité</a></li>
<li class="chapter" data-level="2.3.4" data-path="methodes.html"><a href="methodes.html#profils-de-diversite"><i class="fa fa-check"></i><b>2.3.4</b> Profils de diversité</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="methodes.html"><a href="methodes.html#la-decomposition-de-lentropie"><i class="fa fa-check"></i><b>2.4</b> La décomposition de l’entropie</a></li>
<li class="chapter" data-level="2.5" data-path="methodes.html"><a href="methodes.html#diversite-jointe-information-mutuelle-et-redondance"><i class="fa fa-check"></i><b>2.5</b> Diversité jointe: information mutuelle et redondance</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="concentration-spatiale-et-specialisation.html"><a href="concentration-spatiale-et-specialisation.html"><i class="fa fa-check"></i><b>3</b> Concentration spatiale et spécialisation</a><ul>
<li class="chapter" data-level="3.1" data-path="concentration-spatiale-et-specialisation.html"><a href="concentration-spatiale-et-specialisation.html#concentration-spatiale"><i class="fa fa-check"></i><b>3.1</b> Concentration spatiale</a><ul>
<li class="chapter" data-level="3.1.1" data-path="concentration-spatiale-et-specialisation.html"><a href="concentration-spatiale-et-specialisation.html#ubiquite-et-concentration-absolue"><i class="fa fa-check"></i><b>3.1.1</b> Ubiquité et concentration absolue</a></li>
<li class="chapter" data-level="3.1.2" data-path="concentration-spatiale-et-specialisation.html"><a href="concentration-spatiale-et-specialisation.html#concentration-relative"><i class="fa fa-check"></i><b>3.1.2</b> Concentration relative</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="concentration-spatiale-et-specialisation.html"><a href="concentration-spatiale-et-specialisation.html#specialisation"><i class="fa fa-check"></i><b>3.2</b> Spécialisation</a></li>
<li class="chapter" data-level="3.3" data-path="concentration-spatiale-et-specialisation.html"><a href="concentration-spatiale-et-specialisation.html#tests-de-significativite"><i class="fa fa-check"></i><b>3.3</b> Tests de significativité</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="diversite-jointe.html"><a href="diversite-jointe.html"><i class="fa fa-check"></i><b>4</b> Diversité jointe</a><ul>
<li class="chapter" data-level="4.1" data-path="diversite-jointe.html"><a href="diversite-jointe.html#diversite-specialisation-des-pays"><i class="fa fa-check"></i><b>4.1</b> Diversité (spécialisation) des pays</a></li>
<li class="chapter" data-level="4.2" data-path="diversite-jointe.html"><a href="diversite-jointe.html#ubiquite-concentration-des-secteurs"><i class="fa fa-check"></i><b>4.2</b> Ubiquité (concentration) des secteurs</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mesure de la biodiversité et de la structuration spatiale de l’activité économique par l’entropie</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="methodes" class="section level1">
<h1><span class="header-section-number">2</span> Méthodes</h1>
<div id="questions-similaires-et-notions-opposees" class="section level2">
<h2><span class="header-section-number">2.1</span> Questions similaires et notions opposées</h2>
<p>Les méthodes présentées ici ont été développées par la littérature sur la biodiversité. Les écologues ont besoin de mesurer la <em>diversité</em> d’une communauté végétale, composée de plusieurs espèces dont les effectifs sont connus. Une question moins traitée concerne l’<em>ubiquité</em> des espèces, c’est-à-dire pour une espèce donnée la diversité des environnements dans lesquels elle se trouve. Cette notion a été formalisée sous le nom de largeur de niche par <span class="citation">Levins (1968)</span>, au sens où la niche écologique est l’ensemble des conditions nécessaires au développement et à la reproduction d’un être vivant. Pour fixer les idées et sans perte de généralité, les exemples traités ici concerneront des arbres dans une forêt tropicale. Chaque arbre appartient à une et une seule espèce, et le nombre d’individus de chaque espèce est connu. Les arbres ont une taille qui permet de pondérer leur importance: la mesure classiquement utilisée est la surface terrière, c’est-à-dire la surface (horizontale) du tronc découpé à 1,30 m de hauteur <span class="citation">(Kershaw et al. 2017)</span>. Les espèces sont situées dans une taxonomie: elles sont regroupées par genres et les genres par familles. Enfin, la forêt est divisée géographiquement en parcelles, elles-mêmes en sous-parcelles.</p>
<p>En économie géographique, la question probablement la plus traitée est celle de la <em>concentration spatiale</em> <span class="citation">(Ottaviano et Puga 1998; Combes et Gobillon 2015)</span>, source d’externalités positives <span class="citation">(Baldwin et Martin 2004)</span>. Elle est très semblable à l’ubiquité des espèces des écologues, mais opposée: une forte concentration est synonyme d’une faible ubiquité. La <em>spécialisation</em> <span class="citation">(Amiti 1997)</span> est de même la notion inverse de la diversité. Les exemples traités ici en économie concerneront les établissements industriels des pays d’Europe fournis par la base EuroStat en accès libre. Les établissements ont un nombre d’employés, qui permet leur pondération. Ils appartiennent à un secteur d’activité, ici selon la nomenclature NUTS, qui est une taxonomie similaire à celle des espèces biologiques, et leur localisation par pays peut être détaillée par régions (selon la nomenclature NACE) et leurs subdivisions.</p>
<p>La spécialisation et la concentration spatiale <span class="citation">(Cutrini 2010)</span>, comme la diversité et l’ubiquité <span class="citation">(Gregorius 2010)</span> sont mathématiquement liées: l’existence de secteurs très concentrés implique celle de régions spécialisées dans ce secteur. Une approche synthétique peut être développée: <span class="citation">Cutrini (2010)</span> définit la localisation globale à cet effet, qui sera généralisée.</p>
</div>
<div id="donnees-et-notations" class="section level2">
<h2><span class="header-section-number">2.2</span> Données et notations</h2>
<p>Les données ont été choisies pour leur accessibilité et leur simplicité: il s’agit ici de présenter des méthodes plus que de traiter en détail des questions économiques complexes. Les applications s’appuieront sur les nombres de personnes employées par secteur industriel dans 25 pays européens en 2015. Les données sont disponibles en ligne sur la base EuroStat,<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> dans le fichier <em>SBS data by NUTS 2 regions and NACE Rev. 2</em>.</p>
<p>La nomenclature des secteurs économiques est la NACE (Nomenclature statistique des Activités économiques dans les Communautés Européennes) dans sa révision 2. Seuls les secteurs industriels (code NACE : C) ont été retenus. Les secteurs C12 (manufacture de produits du tabac), C19 (manufacture de coke et produits du pétrole raffiné), C21 (Manufacture de produits pharmaceutiques de base et préparations pharmaceutiques) et C30 (Manufacture d’autres équipements de transport) ont été retirés parce qu’ils présentaient des données manquantes dans des pays majeurs (par exemple, C30 en Belgique).</p>
<p>Parmi les 30 pays disponibles, Chypre, Malte, l’Irlande, le Luxembourg et la Slovénie ont été retirés parce qu’ils comportaient trop de données manquantes. La sélection des données se résume donc à un compromis pour conserver l’essentiel de l’information, tout à fait discutable mais suffisant pour les besoins de démonstration méthodologique de cet article.</p>
<p>Après filtrage, les données se présentent donc sous la forme d’une table (appelée tableau de contingence) dont les 19 lignes sont les secteurs industriels et les 25 colonnes les pays retenus. Chaque cellule du tableau contient le nombre de personnes employées dans le secteur et le pays considéré, sans données manquantes.</p>
<p>Les secteurs sont indicés par la lettre <span class="math inline">\(s\)</span> et les pays par la lettre <span class="math inline">\(i\)</span>. Les effectifs par secteur et pays sont notés <span class="math inline">\(n_{s,i}\)</span>. Les valeurs marginales sont notées <span class="math inline">\(n_i\)</span> (l’effectif du pays <span class="math inline">\(i\)</span>, tous secteurs confondus) et <span class="math inline">\(n_s\)</span> (celui du secteur <span class="math inline">\(s\)</span>, tous pays confondus). Pour alléger l’écriture, le niveau d’agrégation correspondant à l’ensemble des secteurs sera appelé l’industrie et celui correspondant à l’ensemble des pays l’Europe: <span class="math inline">\(n_s\)</span> sera donc appelé le nombre de personnes travaillant dans le secteur <span class="math inline">\(s\)</span> en Europe. L’effectif total est <span class="math inline">\(n=\sum_s{n_s}=\sum_i{n_i}\)</span>, égal à 27 419 407. Les tailles relatives des pays et des secteurs sont représentées en annexe. La probabilité qu’une personne choisie au hasard travaille dans le secteur <span class="math inline">\(s\)</span> et le pays <span class="math inline">\(i\)</span> est notée <span class="math inline">\(p_{s,i}\)</span> et estimée par sa fréquence observée <span class="math inline">\(p_{s,i}=n_{s,i}/n\)</span> (pour alléger la notation, la fréquence empirique est notée comme la probabilité théorique plutôt que <span class="math inline">\(\hat{p}_{s,i}\)</span>). Enfin, les probabilités seront aussi considérées par secteur ou par région: <span class="math inline">\(p_{s|i}= p_{s,i}/p_i\)</span> est la probabilité pour une personne du pays <span class="math inline">\(i\)</span> de travailler dans le secteur <span class="math inline">\(s\)</span> dont la somme sur tous les secteurs vaut 1 (<span class="math inline">\(\sum_s{p_{s|i}}=1\)</span>). Le vecteur des probabilités <span class="math inline">\(p_{s|i}\)</span> de tous les secteurs dans le pays <span class="math inline">\(i\)</span> est noté <span class="math inline">\(\mathbf{p_{s|i}}\)</span>. De même, <span class="math inline">\(p_{i|s}\)</span> est la probabilité, dans le secteur <span class="math inline">\(s\)</span> choisi, qu’une personne travaille dans le pays <span class="math inline">\(i\)</span> et <span class="math inline">\(\mathbf{p_{i|s}}\)</span> est le vecteur des probabilité des pays pour le secteur <span class="math inline">\(s\)</span>. Enfin, les probabilités marginales sont notées <span class="math inline">\(p_s\)</span> et <span class="math inline">\(p_i\)</span>; elles sont estimées respectivement par <span class="math inline">\(n_{s}/n\)</span> et <span class="math inline">\(n_{i}/n\)</span>.</p>
<p>Les données et le code R <span class="citation">(R Core Team 2018)</span> nécessaires pour reproduire l’intégralité des résultats se trouvent en annexe. Le code utilise largement le package <em>entropart</em> <span class="citation">(Marcon et Hérault 2015b)</span> consacré à la mesure de la biodiversité.</p>
</div>
<div id="lentropie-comme-mesure-dincertitude" class="section level2">
<h2><span class="header-section-number">2.3</span> L’entropie comme mesure d’incertitude</h2>
<p>Les notions étant établies, il s’agit maintenant de les traduire en mesures opérationnelles permettant de comparer la diversité de différentes communautés végétales ou la spécialisation de régions industrielles, de donner un sens concret, facilement compréhensible, à ces mesures, et de caractériser leurs propriétés pour pouvoir les utiliser par exemple dans le cadre de modèles.</p>
<p>La diversité biologique est un déterminant important du fonctionnement des écosystèmes <span class="citation">(Chapin et al. 2000)</span>. Parmi de très nombreuses mesures <em>ad-hoc</em> développées selon les besoins <span class="citation">(Peet 1974)</span>, l’intérêt de l’entropie de <span class="citation">Shannon (1948)</span> a été argumenté notamment par <span class="citation">Pielou (1975)</span> dans un ouvrage de référence. En économétrie, les travaux de <span class="citation">Davis (1941)</span> et surtout <span class="citation">Theil (1967)</span> ont ouvert la voie. Le très connu indice de Theil est la différence entre l’entropie de Shannon et sa valeur maximale possible, ce qui illustre l’opposition des approches présentée plus haut en même temps que la convergence des méthodes.</p>
<p>L’entropie est, entre autres, une mesure d’incertitude qu’il est temps de formaliser. Définissons une expérience (par exemple l’échantillonnage d’un arbre au hasard dans une forêt) dont l’ensemble des résultats possibles (l’espèce à laquelle il appartient) est connu. Les résultats sont notés <span class="math inline">\(r_s\)</span> où l’indice <span class="math inline">\(s\)</span> prend toutes les valeurs possibles entre 1 et <span class="math inline">\(S\)</span>, le nombre de résultats possibles. La probabilité d’obtenir <span class="math inline">\(r_s\)</span> est <span class="math inline">\(p_s\)</span>, et <span class="math inline">\(\mathbf{p_s}=(p_1,p_2,\dots,p_S)\)</span> est l’ensemble (mathématiquement, le vecteur) des probabilités d’obtenir chaque résultat. L’obtention du résultat <span class="math inline">\(r_s\)</span> est peu étonnante si <span class="math inline">\(p_s\)</span> est grande: elle apporte peu d’information supplémentaire par rapport à la simple connaissance des probabilités. En revanche, si l’espèce <span class="math inline">\(r_s\)</span> est rare (<span class="math inline">\(p_s\)</span> est petite), son tirage est surprenant. La notion d’information, définie par Shannon, est identique à celle de surprise, plus intuitive. On définit donc une fonction d’information, <span class="math inline">\(I(p_s)\)</span>, décroissante quand la probabilité augmente, de <span class="math inline">\(I(0)=+\infty\)</span> (ou éventuellement une valeur strictement positive finie) à <span class="math inline">\(I(1)=0\)</span> (l’observation d’un résultat certain n’apporte aucune surprise).</p>
<p>L’entropie est définie comme la moyenne de l’information apportée par tous les résultats possibles de l’expérience. Comme chaque résultat à la probabilité <span class="math inline">\(p_s\)</span> d’être réalisée, la moyenne sur tous les résultats possibles est la moyenne pondérée de <span class="math inline">\(I(p_s)\)</span>. L’entropie est définie comme <span class="math display">\[H(\mathbf{p_s})=\sum_{s}{p_s I(p_s)}.\]</span></p>
<div id="entropie-de-shannon" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Entropie de Shannon</h3>
<p>Shannon a utilisé la fonction d’information <span class="math inline">\(I(p_s)=-\ln{p_s}\)</span> pour ses propriétés mathématiques. Elle peut être écrite sous la forme <span class="math inline">\(I(p_s)=\ln({1/p_s})\)</span>. L’inverse de la probabilité, <span class="math inline">\(1/p_s\)</span>, sera appelé <em>rareté</em>: une espèce très rare a une probabilité proche de 0. La fonction d’information utilisée par Shannon est donc le logarithme de la rareté.</p>
<p>Le terme entropie avait été introduit par Clausius en 1865 pour sa nouvelle formulation du second principe de la thermodynamique énoncé par Carnot 40 ans plus tôt. Son étymologie grecque signifie <em>transformation</em> parce que le second principe concerne la variation d’entropie. Boltzmann a caractérisé l’entropie d’un système complexe (un gaz, donc chaque particule peut avoir plusieurs états possibles) en 1877 <span class="citation">(Sharp et Matschinsky 2015)</span>. <span class="citation">Shannon (1948)</span> a enfin montré que le nombre d’états possibles d’un système est analogue au nombre de messages d’une longueur choisie pouvant être créés en assemblant les lettres d’un alphabet dont les fréquences des lettres sont fixées. L’entropie de Shannon est, à une constante près, égale à celle de Boltzmann normalisée par la longueur du message, dont elle est indépendante. Cette propriété fondamentale lui permet de décrire la complexité d’un système non seulement par le nombre possible de ses états, mais plus simplement par la fréquence relative de ses composants, donnant naissance à la théorie de l’information.</p>
<p>La pertinence de l’entropie comme mesure de diversité en découle directement: un système est d’autant plus divers qu’il peut avoir un grand nombre d’état possibles ou, de manière équivalente, qu’il est difficile de prévoir l’état dans lequel il se trouve, ou encore qu’il a une entropie élevée.</p>
</div>
<div id="entropie-generalisee" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Entropie généralisée</h3>
<p>De nombreuses fonctions d’informations alternatives sont envisageables, y compris les plus exotiques comme <span class="math inline">\(I(p_s)=\cos({p_s \pi/2})\)</span> <span class="citation">(Gregorius 2014)</span>.</p>
<p>Parmi elles, trois familles de fonctions paramétrisables se sont imposées : l’entropie généralisée de la littérature des inégalités <span class="citation">(Shorrocks 1980)</span>, l’entropie de <span class="citation">Rényi (1961)</span>, très utilisée jusqu’aux années 2000 pour la mesure de la biodiversité et, plus récemment, l’entropie HCDT détaillée ici.</p>
<p><span class="citation">Tsallis (1988)</span> a proposé une cette entropie généralisée en physique statistique pour des systèmes ne répondant pas aux propriétés nécessaires à la théorie de Boltzmann. Elle avait été définie par <span class="citation">Havrda et Charvát (1967)</span> en cybernétique et redécouverte ensuite, notamment par <span class="citation">Daróczy (1970)</span> en théorie de l’information, d’où son nom, entropie <em>HCDT</em> (voir <span class="citation">Mendes et al. (2008)</span>, page 451, pour un historique complet).</p>
<p>Sa forme mathématique est: <span class="math display">\[ ^{q}H(\mathbf{p_s}) = \frac{1}{q-1}\left(1-\sum^S_{s=1}{p^q_s}\right),\]</span> où <span class="math inline">\(q\)</span> est un paramètre arbitraire. <span class="math inline">\(^{1}H\)</span> n’est pas définie directement mais on montre que <span class="math inline">\(^{q}H\)</span> tend vers l’entropie de Shannon quand <span class="math inline">\(q \to 1\)</span>.</p>
<p>Son intérêt apparaît plus clairement en définissant une généralisation de la fonction logarithme, le logarithme déformé d’ordre <span class="math inline">\(q\)</span> <span class="citation">(Tsallis 1994)</span> comme <span class="math display">\[\ln_q{x} = \frac{x^{1-q}-1}{1-q}.\]</span> Ici encore, <span class="math inline">\(\ln_q{x}\)</span> tend vers le logarithme naturel quand <span class="math inline">\(q\)</span> tend vers 1. L’entropie HCDT s’écrit alors comme une généralisation de l’entropie de Shannon : <span class="math display">\[^{q}H(\mathbf{p_s}) = \sum_{s}{p_s \ln_q{(1/p_s)}}\]</span></p>
<p>Le logarithme déformé est une fonction qui, comme son nom l’indique, déforme la fonction logarithme naturel en changeant sa courbure mais en respectant, quel que soit <span class="math inline">\(q\)</span>, <span class="math inline">\(\ln_q{1}=0\)</span> et les limites (<span class="math inline">\(-\infty\)</span> quand <span class="math inline">\(x \to 0\)</span> et <span class="math inline">\(+\infty\)</span> quand <span class="math inline">\(x \to \infty\)</span>). En faisant varier le paramètre <span class="math inline">\(q\)</span>, la fonction d’information <span class="math inline">\(\ln_q{(1/p_s)}\)</span> attribue une plus grande (quand q, supérieur à 1, croît) ou moins grande (quand q, inférieur à 1, décroît) surprise aux espèces rares (dont la rareté, <span class="math inline">\(1/p_s\)</span>, est grande).</p>
<p>On dispose à ce stade d’une définition simple et générale : l’entropie (d’ordre <span class="math inline">\(q\)</span>) d’un système est la surprise moyenne apportée par l’observation d’un de ses individus; la surprise est le logarithme (d’ordre <span class="math inline">\(q\)</span>) de la rareté. Une communauté biologique est d’autant plus diverse qu’elle est surprenante (que son entropie est grande). Une région est d’autant plus spécialisée que son entropie est faible.</p>
<p>Trois valeurs de <span class="math inline">\(q\)</span> sont particulièrement intéressantes:</p>
<ul>
<li><p><span class="math inline">\(q=0\)</span>: l’entropie est la richesse, c’est-à-dire <span class="math inline">\(S\)</span>, le nombre d’espèces ou de secteurs, moins 1 ;</p></li>
<li><p><span class="math inline">\(q=1\)</span>: l’entropie est celle de Shannon. En économétrie, <span class="math inline">\(S-^{1}H\)</span> est l’indice de Theil;</p></li>
<li><p><span class="math inline">\(q=2\)</span>: l’entropie est l’indice de biodiversité de <span class="citation">Simpson (1949)</span>, c’est-à-dire la probabilité que deux individus choisis au hasard appartiennent à une espèce différente. En économétrie, son complément à 1, c’est à dire la probabilité que deux individus appartiennent au même secteur, est l’indice de Herfindahl, ou Herfindahl-Hirschman <span class="citation">(Hirschman 1964)</span>, qui mesure ici la spécialisation.</p></li>
</ul>
<p>Les valeurs négatives de <span class="math inline">\(q\)</span> donnent à une espèce une importance d’autant plus grande qu’elle est rare alors qu’à <span class="math inline">\(q=0\)</span> toutes les espèces contribuent de façon identique à l’entropie (elles sont simplement comptées, quelle que soit leur probabilité). Leur intérêt est donc limité. Comme leurs propriétés mathématiques sont mauvaises <span class="citation">(Marcon et al. 2014)</span>, elles ne sont en pratique pas utilisées. Les valeurs de <span class="math inline">\(q\)</span> supérieures à 2 sont peu utilisées parce qu’elles négligent trop les espèces qui ne sont pas les plus fréquentes.</p>
</div>
<div id="de-lentropie-a-la-diversite" class="section level3">
<h3><span class="header-section-number">2.3.3</span> De l’entropie à la diversité</h3>
<p>L’entropie a un sens physique : c’est une quantité de surprise; c’est donc bien plus qu’un indice, qui n’est qu’une valeur arbitraire devant seulement respecter une relation d’ordre pour permettre des comparaisons. Cependant, à l’exception des ordres 0 et 2, la valeur de l’entropie n’a pas d’interprétation intuitive. Les nombres de Hill répondent à ce manque.</p>
<p>Le souhait de <span class="citation">Hill (1973)</span> était de rendre les indices de diversité intelligibles après l’article remarqué de <span class="citation">Hurlbert (1971)</span> intitulé le non-concept de diversité spécifique. Hurlbert reprochait à la littérature sur la diversité sa trop grande abstraction et son éloignement des réalités biologiques, notamment en fournissant des exemples dans lesquels l’ordre des communautés n’était pas le même selon l’indice de diversité choisi.</p>
<p>Les nombres de Hill sont le nombre d’espèces équiprobables donnant la même valeur d’entropie que la distribution observée, autrement dit des <em>nombres effectifs</em> d’espèces, encore appelés <em>nombres équivalents</em>. Le concept a été défini rigoureusement par <span class="citation">Gregorius (1991)</span>, d’après <span class="citation">Wright (1931)</span> qui avait le premier défini la taille effective d’une population en génétique: étant donné une variable caractéristique (ici, l’entropie) fonction seulement d’une variable numérique (ici, le nombre d’espèces) dans un cas idéal (ici, l’équiprobabilité des espèces), le nombre effectif est la valeur de la variable numérique pour laquelle la variable caractéristique est celle du jeu de données.</p>
<p>Formellement, ils sont simplement l’exponentielle déformée de l’entropie HCDT <span class="citation">(Marcon et al. 2014)</span>. La fonction exponentielle déformée d’ordre <span class="math inline">\(q\)</span> est la fonction réciproque du logarithme déformé, dont la valeur est <span class="math display">\[e^x_q = [1 + (1-q)x]^{1/(1-q)}.\]</span></p>
<p>Le nombre de Hill d’ordre <span class="math inline">\(q\)</span>, appelé simplement <em>diversité d’ordre <span class="math inline">\(q\)</span></em> <span class="citation">(Jost 2006)</span> est donc <span class="math display">\[^{q}D(\mathbf{p_s}) = e_q^{^{q}H(\mathbf{p_s})}.\]</span></p>
<p>La formulation explicite à partir des probabilités est: <span class="math display">\[^{q}D(\mathbf{p_s}) = \left( \sum_{s}{p^q_s} \right)^{1/(1-q)}.\]</span> Ces résultats avaient déjà été obtenus avec une autre approche par <span class="citation">MacArthur (1965)</span> et repris par <span class="citation">Adelman (1969)</span> dans la littérature économique. Aussi, la mesure d’inégalité <a href="mailto:d&#39;@Atkinson1970">d'@Atkinson1970</a> est très similaire aux nombres de Hill.</p>
</div>
<div id="profils-de-diversite" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Profils de diversité</h3>
<p>La diversité étant exprimée dans la même unité (un nombre d’espèces) quel que soit son ordre, il est possible de tracer un profil de diversité, c’est-à-dire la valeur de <span class="math inline">\(^{q}D\)</span> en fonction de <span class="math inline">\(q\)</span>. Les courbes de deux communautés peuvent se croiser parce que le poids des espèces rares diminue avec l’augmentation de <span class="math inline">\(q\)</span>. Si ce n’est pas le cas, la relation d’ordre entre les communautés est bien définie <span class="citation">(Tothmeresz 1995)</span>.</p>
</div>
</div>
<div id="la-decomposition-de-lentropie" class="section level2">
<h2><span class="header-section-number">2.4</span> La décomposition de l’entropie</h2>
<p>La notion de diversité <span class="math inline">\(\beta\)</span> a été introduite par <span class="citation">Whittaker (1960)</span> comme le degré de différenciation des communautés biologiques. La question traitée est celle de la décomposition de la diversité de données agrégées (la diversité des secteurs économiques en Europe) à un niveau plus détaillé (par pays). La diversité du niveau le plus agrégé a été appelée <span class="math inline">\(\gamma\)</span> par Whittaker, la diversité moyenne des niveaux détaillés <span class="math inline">\(\alpha\)</span>, et la différenciation entre les niveaux détaillés <span class="math inline">\(\beta\)</span>. Il est évident que les diversités <span class="math inline">\(\gamma\)</span> et <span class="math inline">\(\alpha\)</span> sont de même nature, seul le niveau de détail des données diffère. En revanche, la caractérisation de la diversité <span class="math inline">\(\beta\)</span> a généré des controverses <span class="citation">(Ellison 2010)</span>.</p>
<p>En économie, la décomposition des mesures d’inégalité a suivi une voie parallèle à celle des écologues <span class="citation">(Bourguignon 1979)</span>. Celle de la concentration spatiale est restée limitée à l’entropie de Theil <span class="citation">(Mori, Nishikimi, et Smith 2005; Cutrini 2010)</span> à l’exception notable de <span class="citation">Brülhart et Traeger (2005)</span> qui ont utilisé l’entropie généralisée de <span class="citation">Shorrocks (1980)</span>.</p>
<p><span class="citation">Jost (2007)</span> a montré que la décomposition de l’entropie est additive : l’entropie <span class="math inline">\(\beta\)</span> est la différence entre les entropies <span class="math inline">\(\gamma\)</span> et <span class="math inline">\(\alpha\)</span>. <span class="citation">Marcon et al. (2012)</span> ont ensuite interprété l’entropie <span class="math inline">\(\beta\)</span> comme l’information supplémentaire apportée par la connaissance des distributions désagrégées en plus de celle des données agrégées, c’est-à-dire une entropie relative. Cette information est la divergence entre les distributions, c’est-à-dire une mesure mathématique de l’écart entre leurs valeurs. La divergence de <span class="citation">Kullback et Leibler (1951)</span> est bien connue des économistes sous le nom d’entropie relative de Theil <span class="citation">(Conceição et Ferreira 2000)</span>. La différence entre l’entropie <span class="math inline">\(\gamma\)</span> d’ordre 1 et la moyenne des entropies d’ordre 1 des distributions désagrégées est la moyenne des divergences de Kullback-Leibler correspondantes, appelée par les physiciens statistiques divergence de Jensen-Shannon. <span class="citation">Marcon et al. (2014)</span> ont généralisé ce résultat à tous les ordres de l’entropie HCDT.</p>
<p>Comme l’entropie <span class="math inline">\(\gamma\)</span> et <span class="math inline">\(\alpha\)</span>, l’entropie <span class="math inline">\(\beta\)</span> peut être transformée en un nombre effectif qui est le nombre de communautés de même poids, sans espèce commune, qui auraient la même entropie <span class="math inline">\(\beta\)</span> que les communautés réelles. La décomposition de la diversité est multiplicative : la diversité <span class="math inline">\(\gamma\)</span> est le produit des diversités <span class="math inline">\(\alpha\)</span> et <span class="math inline">\(\beta\)</span>.</p>
<p>La décomposition complète est finalement un produit de nombres effectifs : la diversité de l’assemblage de plusieurs communautés biologiques, appelée diversité <span class="math inline">\(\gamma\)</span> est un nombre effectif d’espèces; c’est le produit du nombre effectif d’espèces de chaque communauté (diversité <span class="math inline">\(\alpha\)</span>) par le nombre effectif de communautés (diversité <span class="math inline">\(\beta\)</span>). Elle sera appliquée dans cet article à l’économie des pays européens : le nombre effectif de secteurs économiques de l’Europe (<span class="math inline">\(\gamma\)</span>) est le produit du nombre effectif moyen de secteurs des pays (<span class="math inline">\(\alpha\)</span>) par un nombre effectif de pays (<span class="math inline">\(\beta\)</span>).</p>
<p>De même, l’ubiquité d’un secteur économique aggrégé (l’industrie manufacturière) est un nombre effectif de pays (<span class="math inline">\(\gamma\)</span>), décomposable en un nombre effectif de pays par secteur désagrégé (<span class="math inline">\(\alpha\)</span>) multiplié par un nombre effectif de secteurs désagrégés (<span class="math inline">\(\beta\)</span>).</p>
<p>La décomposition sera limitée ici à un seul niveau de désagrégation des données. Elle peut être répétée : les pays peuvent être découpés en régions, les régions en départements… Le nombre effectif de secteurs économiques de l’Europe (<span class="math inline">\(\gamma\)</span>) peut alors être décomposé en un nombre effectif de pays (<span class="math inline">\(\beta_1\)</span>) fois un nombre effectif de régions (<span class="math inline">\(\beta_2\)</span>) fois un nombre effectif de départements (<span class="math inline">\(\beta_3\)</span>) fois un nombre effectif de secteurs par département (<span class="math inline">\(\alpha\)</span>). La décomposition hiérarchique de la diversité a été traitée notamment par <span class="citation">Marcon et al. (2012)</span>; <span class="citation">Richard-Hansen et al. (2015)</span>; <span class="citation">Pavoine, Marcon, et Ricotta (2016)</span>.</p>
</div>
<div id="diversite-jointe-information-mutuelle-et-redondance" class="section level2">
<h2><span class="header-section-number">2.5</span> Diversité jointe: information mutuelle et redondance</h2>
<p>Nous avons vu que l’entropie pouvait être utilisée selon les deux points de vue de la diversité et de l’ubiquité (de façon équivalente: la spécialisation et la concentration spatiale). Les données sont les mêmes et peuvent être représentés dans le tableau de contingence dont les lignes représentent par exemple les secteurs industriels alors que les colonnes représentent les pays, chaque cellule de la table fournissant l’abondance (en nombre d’établissements ou de personnes travaillant) d’un secteur dans un pays.</p>
<p>La diversité des pays est calculée en traitant chaque colonne du tableau, l’ubiquité des secteurs en traitant chaque ligne. La diversité <span class="math inline">\(^{q}D(\mathbf{p_s})\)</span> de l’Europe entière (l’agrégation des pays) est obtenue, comme l’ubiquité des secteurs agrégés <span class="math inline">\(^{q}D(\mathbf{p_i})\)</span>, à partir des des probabilités marginales. La diversité <span class="math inline">\(^{q}D(\mathbf{p_{s,i}})\)</span> de l’ensemble des données, tous secteurs et pays confondus, a un grand intérêt, notamment théorique pour l’entropie de Shannon <span class="citation">(Faddeev 1956; Baez, Fritz, et Leinster 2011)</span>: elle est appelée diversité jointe <span class="citation">(Gregorius 2010)</span>.</p>
<p>La différence entre l’entropie jointe et la somme des entropies marginales (celle de l’ensemble des secteurs et celle de l’ensemble des pays), <span class="math inline">\(^{q}H(\mathbf{p_{s,i}})-^{q}H(\mathbf{p_{s}})-^{q}H(\mathbf{p_{i}})\)</span>, s’appelle l’information mutuelle. L’entropie de Shannon (mais pas l’entropie HCDT d’ordre différent de 1) de deux systèmes indépendants s’additionne: si l’appartenance aux pays est indépendante de l’appartenance aux secteurs, c’est-à-dire si la probabilité <span class="math inline">\(p_{s,i}\)</span> est simplement le produit des probabilités <span class="math inline">\(p_{s}\)</span> et <span class="math inline">\(p_{i}\)</span>, alors l’entropie de Shannon jointe est nulle. En d’autres termes, l’information mutuelle est l’entropie supplémentaire apportée par la non indépendance des lignes et des colonnes du tableau. Elle est égale aux deux entropies <span class="math inline">\(\beta\)</span>, celle de la diversité et celle de l’ubiquité. Ces propriétés ne sont valables que pour l’entropie de Shannon mais ont été utilisées sous différentes formes dans la littérature <span class="citation">(par exemple Cutrini 2009; Chao, Wang, et Jost 2013)</span>.</p>
<p>Quel que soit l’ordre considéré, <span class="citation">Gregorius (2010)</span> a montré que la diversité jointe apporte une information supplémentaire importante sur la distribution des abondances qui n’est pas prise en compte par la décomposition de la diversité déjà présentée. L’exemple de la biodiversité est utilisé ici pour simplifier l’exposé. La diversité <span class="math inline">\(\alpha\)</span> est le nombre d’espèces équiprobables dans une communauté type. La diversité <span class="math inline">\(\beta\)</span> est le nombre de ces communautés types, équiprobables et sans espèce commune. La diversité <span class="math inline">\(\gamma\)</span> est le produit des deux précédentes, un nombre d’espèces équiprobables résultant de l’assemblage des communautés. Chaque espèce n’apparaît que dans une communauté dans cette représentation. La réplication à l’identique des communautés ne modifie pas les diversités <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> et <span class="math inline">\(\gamma\)</span>, c’est même une propriété demandée aux mesures de diversité <span class="citation">(Hill 1973)</span>. En revanche, la diversité jointe est multipliée par le nombre de réplications <span class="citation">(Marcon 2017)</span>: le rapport entre la diversité jointe et la diversité <span class="math inline">\(\beta\)</span> mesure la redondance sous la forme d’un nombre effectif, le nombre de répétitions des communautés.</p>
<p>La redondance n’a que peu d’applications pratiques en écologie parce que les données disponibles sont en général des échantillons des communautés étudiées (des placettes d’inventaire forestier par exemple). Leur redondance reflète l’effort d’échantillonnage, qui est un choix de l’expérimentateur. Lorsque les données sont exhaustives ou, plus généralement, lorsque les probabilités marginales des communautés sont interprétables comme leurs tailles, la redondance est une information aussi importante que la diversité.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p><a href="http://ec.europa.eu/eurostat/web/regions/data/database" class="uri">http://ec.europa.eu/eurostat/web/regions/data/database</a><a href="methodes.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="concentration-spatiale-et-specialisation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
